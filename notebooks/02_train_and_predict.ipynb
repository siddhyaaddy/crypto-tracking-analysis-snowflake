{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kv4sbM7wcaL2",
    "outputId": "0f7f8ac3-60e3-42a0-8231-8b74efdbc461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\n",
      "Collecting optuna\n",
      "  Using cached optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from optuna) (1.15.2)\n",
      "Collecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from optuna) (2.0.29)\n",
      "Requirement already satisfied: tqdm in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Using cached optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.9.0 optuna-4.3.0\n",
      "Collecting snowflake-connector-python\n",
      "  Downloading snowflake_connector_python-3.15.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (70 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3>=1.24 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.38.1)\n",
      "Requirement already satisfied: botocore>=1.24 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.38.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (43.0.0)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (24.2.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (4.13.2)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.18.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.10.0)\n",
      "Requirement already satisfied: tomlkit in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (0.11.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: pycparser in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n",
      "Downloading snowflake_connector_python-3.15.0-cp312-cp312-macosx_11_0_arm64.whl (988 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m988.6/988.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Installing collected packages: asn1crypto, snowflake-connector-python\n",
      "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.15.0\n",
      "Requirement already satisfied: python-dotenv in /Users/siddharthadhikari/anaconda3/lib/python3.12/site-packages (0.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install optuna\n",
    "!pip install snowflake-connector-python\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRysb-0TkGRf",
    "outputId": "828fe485-255b-404a-946c-424a2bc757b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/6940phmn1lz8gtmdwt62nz980000gn/T/ipykernel_54661/1417125309.py:10: DeprecationWarning: This package has been renamed to snowflake_uuid and will be removed shortly. Please update immediately.\n",
      "  import snowflake.connector\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snowflake.connector'; 'snowflake' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load environment variables from .env file\u001b[39;00m\n\u001b[1;32m     13\u001b[0m load_dotenv()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snowflake.connector'; 'snowflake' is not a package"
     ]
    }
   ],
   "source": [
    "# --- 0. Imports and Environment Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import snowflake.connector\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Snowflake Connection Setup ---\n",
    "sf_account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "sf_user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "sf_password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "sf_warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "sf_database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "sf_schema = os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
    "sf_role = os.getenv(\"SNOWFLAKE_ROLE\")\n",
    "\n",
    "def get_snowflake_conn():\n",
    "    return snowflake.connector.connect(\n",
    "        user=sf_user,\n",
    "        password=sf_password,\n",
    "        account=sf_account,\n",
    "        warehouse=sf_warehouse,\n",
    "        database=sf_database,\n",
    "        schema=sf_schema,\n",
    "        role=sf_role,\n",
    "    )\n",
    "\n",
    "# Pickup Location ID\tZone Name\tBorough\n",
    "# 132\tJFK Airport\tQueens\n",
    "# 237\tUpper East Side South\tManhattan\n",
    "# 161\tLincoln Square East\tManhattan\n",
    "# 43\tGrand Central\tManhattan\n",
    "\n",
    "pickup_location_id = 43\n",
    "\n",
    "# --- 2. Feature Engineering Function ---\n",
    "def make_features(df):\n",
    "    df = df.copy()\n",
    "    df['RIDES'] = pd.to_numeric(df['RIDES'], errors='coerce')\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "\n",
    "    rides = df['RIDES'].values\n",
    "    n_hours_in_week = 7 * 24\n",
    "    week_offsets = [n_hours_in_week * i for i in range(1, 5)]\n",
    "    hour_offsets = list(range(-5, 6))\n",
    "\n",
    "    lag_feature_names = []\n",
    "    lag_features = []\n",
    "    for week_num, week_offset in enumerate(week_offsets, 1):\n",
    "        for h_offset in hour_offsets:\n",
    "            lag = week_offset + h_offset\n",
    "            lag_feature_names.append(f'lag_w{week_num}_h{h_offset:+d}')\n",
    "            lag_features.append(np.roll(rides, lag))\n",
    "\n",
    "    lag_features = np.column_stack(lag_features)\n",
    "    lag_df = pd.DataFrame(lag_features, index=df.index, columns=lag_feature_names)\n",
    "\n",
    "    max_lag = max(week_offsets) + max(abs(h) for h in hour_offsets)\n",
    "    lag_df.iloc[:max_lag, :] = np.nan\n",
    "\n",
    "    mean_4w_same_hour = lag_df[[f'lag_w{i}_h+0' for i in range(1, 5)]].mean(axis=1)\n",
    "    df['mean_4w_same_hour'] = mean_4w_same_hour\n",
    "\n",
    "    df = pd.concat([df, lag_df], axis=1)\n",
    "    return df, lag_feature_names\n",
    "\n",
    "# --- 3. Fetch Training Data ---\n",
    "query = f\"\"\"\n",
    "SELECT pickup_hour, rides\n",
    "FROM NYC_DATA.PUBLIC.YELLOW_TAXI_DATA_TRANSFORMED\n",
    "WHERE PICKUP_LOCATION_ID = {pickup_location_id}\n",
    "  AND pickup_hour >= '2023-01-01'\n",
    "  AND pickup_hour < '2024-02-01'\n",
    "ORDER BY pickup_hour\n",
    "\"\"\"\n",
    "\n",
    "conn = get_snowflake_conn()\n",
    "df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "df['PICKUP_HOUR'] = pd.to_datetime(df['PICKUP_HOUR'])\n",
    "df = df.set_index('PICKUP_HOUR').sort_index()\n",
    "\n",
    "# --- 4. Feature Engineering for Training ---\n",
    "df, lag_feature_names = make_features(df)\n",
    "df['target'] = df['RIDES'].shift(-1)\n",
    "df = df.dropna()\n",
    "\n",
    "features = ['RIDES', 'hour', 'dayofweek', 'mean_4w_same_hour'] + lag_feature_names\n",
    "X = df[features].astype(np.float32)\n",
    "y = df['target'].astype(np.float32)\n",
    "\n",
    "# --- 5. Train/Test Split (last 4 weeks as test) ---\n",
    "split_point = df.index.max() - timedelta(weeks=4)\n",
    "X_train, X_test = X[X.index <= split_point], X[X.index > split_point]\n",
    "y_train, y_test = y[y.index <= split_point], y[y.index > split_point]\n",
    "\n",
    "# --- 6. Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    params = {\n",
    "      \"objective\": \"regression\",\n",
    "      \"metric\": \"mae\",\n",
    "      \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 90),\n",
    "      \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.03, log=True),\n",
    "      \"n_estimators\": trial.suggest_int(\"n_estimators\", 350, 900),\n",
    "      \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 80, 130),\n",
    "      \"subsample\": trial.suggest_float(\"subsample\", 0.9, 1.0),\n",
    "      \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.9),\n",
    "      \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.8, 2.0),\n",
    "      \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 2.0),\n",
    "      \"random_state\": 42,\n",
    "      \"verbosity\": -1,\n",
    "      \"n_jobs\": -1\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    return mae\n",
    "\n",
    "# --- 7. Run Optuna and Save Model ---\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200, n_jobs=1)\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train final model on all data\n",
    "final_model = lgb.LGBMRegressor(**best_params, n_jobs=-1)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "preds = final_model.predict(X_test)\n",
    "preds_ceil = np.ceil(preds).astype(int)\n",
    "mae = np.mean(np.abs(preds_ceil - y_test))\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# Save model as pickle\n",
    "model_path = \"lgbm_nyc_taxi.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msGJXiaFkvP4",
    "outputId": "3c382589-df92-47a8-9b9f-14c0601bfebb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-74ba53b672ca>:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch predictions written to Snowflake table: NYC_DATA.PUBLIC.YELLOW_TAXI_DATA_PREDICTIONS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Fetch Data for Batch Prediction ---\n",
    "query = f\"\"\"\n",
    "SELECT pickup_hour, rides\n",
    "FROM NYC_DATA.PUBLIC.YELLOW_TAXI_DATA_TRANSFORMED\n",
    "WHERE PICKUP_LOCATION_ID = {pickup_location_id}\n",
    "  AND pickup_hour >= '2023-12-01'\n",
    "ORDER BY pickup_hour\n",
    "\"\"\"\n",
    "\n",
    "conn = get_snowflake_conn()\n",
    "df = pd.read_sql(query, conn)\n",
    "df['PICKUP_HOUR'] = pd.to_datetime(df['PICKUP_HOUR'])\n",
    "df = df.set_index('PICKUP_HOUR').sort_index()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "df, lag_feature_names = make_features(df)\n",
    "\n",
    "# --- Prepare Features for Prediction ---\n",
    "features = ['RIDES', 'hour', 'dayofweek', 'mean_4w_same_hour'] + lag_feature_names\n",
    "df_features = df[features].astype(np.float32)\n",
    "df_features = df_features.dropna()  # Only predict where all features are available\n",
    "\n",
    "# --- Load Trained Model ---\n",
    "model_path = \"lgbm_nyc_taxi.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# --- Batch Predict ---\n",
    "preds = model.predict(df_features)\n",
    "preds_ceil = np.ceil(preds).astype(int)\n",
    "\n",
    "# --- Prepare Results DataFrame ---\n",
    "prediction_times = df_features.index + pd.Timedelta(hours=1)\n",
    "result_df = pd.DataFrame({\n",
    "    \"pickup_hour\": prediction_times.astype(str),\n",
    "    \"year\": prediction_times.year,\n",
    "    \"month\": prediction_times.month,\n",
    "    \"day\": prediction_times.day,\n",
    "    \"hour\": prediction_times.hour,\n",
    "    \"predicted_rides\": preds_ceil,\n",
    "    \"pickup_location_id\": pickup_location_id\n",
    "})\n",
    "\n",
    "# --- Write Predictions to Snowflake ---\n",
    "table_name = \"NYC_DATA.PUBLIC.YELLOW_TAXI_DATA_PREDICTIONS\"\n",
    "\n",
    "with get_snowflake_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Create the table if it doesn't exist\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                pickup_hour TIMESTAMP,\n",
    "                year INTEGER,\n",
    "                month INTEGER,\n",
    "                day INTEGER,\n",
    "                hour INTEGER,\n",
    "                predicted_rides INTEGER,\n",
    "                pickup_location_id INTEGER\n",
    "            )\n",
    "        \"\"\")\n",
    "        # Delete existing predictions for this pickup_location_id\n",
    "        delete_sql = f\"\"\"\n",
    "            DELETE FROM {table_name}\n",
    "            WHERE pickup_location_id = %s\n",
    "        \"\"\"\n",
    "        cur.execute(delete_sql, (pickup_location_id,))\n",
    "\n",
    "        # Insert all predictions\n",
    "        insert_sql = f\"\"\"\n",
    "            INSERT INTO {table_name} (pickup_hour, year, month, day, hour, predicted_rides, pickup_location_id)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cur.executemany(insert_sql, result_df.values.tolist())\n",
    "\n",
    "print(\"Batch predictions written to Snowflake table:\", table_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
